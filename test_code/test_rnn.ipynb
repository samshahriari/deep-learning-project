{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f712f8-daa3-4dbd-9360-5d9d932c490e",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "The point of the exercise is to construct a simple neural character model that can predict the (n+1)th character, given the n preceding characters.\n",
    "\n",
    "Usually, such language models operate on the word level, but we use a character model because it is simpler and quicker to train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf371b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/imrandiva/Library/Python/3.10/lib/python/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f1aeaf0-3a25-4467-b4d0-61535b30bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b035e-b525-432a-855f-de109266da05",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before.\n",
    "\n",
    "For instance, if the text begins \"Harry Potter\", we want to transform this into $[1, 2, 3, 3, 4, 5, 6, 7, 8, 8, 9, 3, ...]$, where \"H\" has ID 1, \"a\" is 2, \"r\" is 3, etc. (ID 0 is reserved for the special padding symbol, so we start numbering from 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63e58f36-84ad-45a7-9260-8e714a1a2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to init mappings from characters to IDs and back again\n",
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0  # ID 0 is reserved for <PAD>\n",
    "id_to_char.append(PADDING_SYMBOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e81d0246-7a35-446e-8cf6-33e130b545c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this function \n",
    "def string_to_ids(string) :\n",
    "    \"\"\"\n",
    "    Translate this string into a list of character IDs.\n",
    "    The IDs will be integers 1,2,..., and created as needed.\n",
    "    \"\"\"\n",
    "    chars_ids = []  # This list will hold the result \n",
    "\n",
    "    for char in string:\n",
    "        # YOUR CODE HERE\n",
    "        if char not in char_to_id:\n",
    "            char_to_id[char] = len(id_to_char)\n",
    "            id_to_char.append(char)\n",
    "        chars_ids.append(char_to_id[char])\n",
    "\n",
    "    return chars_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e94e393-2e80-4980-8fe7-fc1b766a3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "with open('goblet_book.txt', 'r', encoding='utf-8') as f:\n",
    "    contents = f.read() \n",
    "chars_ids = string_to_ids(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ebae1-9f82-4c44-8f36-2dc936f3f785",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.\n",
    "\n",
    "The init function reads a training text, and slides over it, creating chunks $n$ characters long. These chunks will be our data points, and the corresponding $(n+1)$th character will be the label.\n",
    "\n",
    "For instance, if $n=4$, and the text begins \"Harry P\", which corresponds to the IDs $1,2,3,3,4,5,6$, then the first data point will be $[1,2,3,3]$ and its label is $4$, the second data point is $[2,3,3,4]$ with label $5$, and the third data point is $[3,3,4,5]$ with label $6$.\n",
    "\n",
    "To extend the 'Dataset' class, the CharDataset class has to implement the __len__ and __getitem__ methods, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2f4d7c88-15d2-47ad-8242-4a94aa826ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this class definition\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    datapoints = []  # Each datapoint is a sequence of n characters\n",
    "    labels = []  # The corresponding label is the character that comes next\n",
    "\n",
    "    def __init__(self, file_path, n):\n",
    "        \"\"\"\n",
    "        'file_path' is the name of the training data file\n",
    "\n",
    "        'n' is the number of consecutive characters the model will look at\n",
    "        to predict which letter comes next\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read() \n",
    "        chars_ids = string_to_ids(contents)\n",
    "\n",
    "        # Go through the chars_ids and create data points and labels\n",
    "        # YOUR CODE HERE\n",
    "        for i in range(len(chars_ids)-n-1):\n",
    "            self.datapoints.append(chars_ids[i:i+n])\n",
    "            self.labels.append(chars_ids[i+n])\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f71171f5-5c9c-4a6c-b5ab-f7112daba56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def char_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the longest sequence in the batch.\n",
    "\n",
    "    'batch' is a list of tuples [(datapoint, label), ...]\n",
    "\n",
    "    Returns a tuple of:\n",
    "            - Padded datapoints as a tensor\n",
    "            - Labels as a tensor \n",
    "   \"\"\"\n",
    "    datapoints, labels = zip(*batch)  \n",
    "    padded_datapoints = pad_sequence(datapoints, batch_first=True)\n",
    "    return padded_datapoints, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e6c95-d1bd-4383-8e54-ce0ec1158130",
   "metadata": {},
   "source": [
    "#### Create a neural network according to the following specification:\n",
    "\n",
    "The hyperparameters are:\n",
    "* n -- the number of characters to input (to predict character n+1)\n",
    "* h -- the number of neurons in the hidden layer\n",
    "* v -- the number of unique characters\n",
    "\n",
    "The network should have:\n",
    "1. an embedding layer, mapping character IDs to h-dimensional vectors\n",
    "2. a hidden layer with a linear transformation of size $(nh)\\times(nh)$, followed by a tanh application\n",
    "3. a final layer with a linear transformation of size $(nh)\\times v$\n",
    "\n",
    "The input to the forward pass is a tensor of character IDs $x$ of shape $(\\mathtt{batch\\_size} \\times n)$. The forward pass should:\n",
    "1. Map $x$ to a tensor of character embeddings of shape $(\\mathtt{batch\\_size} \\times n \\times h)$\n",
    "2. Reshape that tensor to shape $(\\mathtt{batch\\_size} \\times nh)$\n",
    "3. Apply the hidden layer (linear transformation and the tanh operation)\n",
    "4. Apply the final layer\n",
    "5. Return the result of the last operation\n",
    "\n",
    "Before starting the implementation, have a look at the documentation for:\n",
    " - `torch.nn.Embedding`\n",
    " - `torch.nn.Linear`\n",
    " - `torch.tanh`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "392ba45c-1a18-4e81-8d7f-7ea3d9f33470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n, h, v):\n",
    "        super(CharModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.v = v\n",
    "        self.h = h\n",
    "        self.embed = nn.Embedding(v, h)\n",
    "        self.rnn = nn.RNN(h, h, batch_first=True) \n",
    "        self.final = nn.Linear(n*h, v)\n",
    "\n",
    "    def forward(self, x, h0= None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.rnn(x, h0)  # RNN layer\n",
    "        out = out.reshape(batch_size, self.n*self.h)\n",
    "        out = self.final(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bff77c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n, h, v):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.v = v\n",
    "        self.h = h\n",
    "        self.embed = nn.Embedding(v, h)\n",
    "        self.lstm = nn.LSTM(h, h, batch_first=True) \n",
    "        self.final = nn.Linear(n*h, v)\n",
    "\n",
    "    def forward(self, x, h0= None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.lstm(x, h0) # LSTM layer\n",
    "        out = out.reshape(batch_size, self.n*self.h)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, model, x, y, lr):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss = criterion(x.squeeze(1), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.detach().item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7192cd-6e9a-4cb1-8840-ca5973658f0d",
   "metadata": {},
   "source": [
    "Next, we will train a model with n=8, i.e. the model will try to predict the 9th character based on the 8 preceding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1efb5e1f-1f4c-4d1f-aee1-1f1857f98aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "There are 1107509 datapoints and 81 unique characters in the dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(input_tensor)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted : \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss=\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "Cell \u001b[0;32mIn[103], line 23\u001b[0m, in \u001b[0;36mLSTMModel.backward_pass\u001b[0;34m(self, model, x, y, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), y)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Choose 'Run all cells' in the 'Run' menu to run this cell.\n",
    "_ = torch.manual_seed(21)\n",
    "\n",
    "# ===================== Hyperparameters ================== #\n",
    "\n",
    "n = 32\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "number_of_epochs = 5\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "training_dataset = CharDataset('goblet_book.txt', n)\n",
    "print( \"There are\", len(training_dataset), \"datapoints and\", len(id_to_char), \"unique characters in the dataset\" ) \n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, collate_fn=char_collate_fn, shuffle=True)\n",
    "model = LSTMModel(n, hidden_size, len(char_to_id)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# model.train()\n",
    "# # print(datetime.now().strftime(\"%X\"), \"Training starts\")\n",
    "# # for epoch in range(number_of_epochs) :\n",
    "# #     for input_tensor, label in training_loader:\n",
    "# #         input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "\n",
    "# #         charlm_optimizer.zero_grad()\n",
    "\n",
    "# #         # Forward pass happening here\n",
    "# #         logits = model(input_tensor).to(device)\n",
    "        \n",
    "# #         # Compute the loss\n",
    "# #         loss = criterion(logits.squeeze(1), label)\n",
    "# #         loss.backward()\n",
    "\n",
    "# #         # Update the weights\n",
    "# #         charlm_optimizer.step()\n",
    "\n",
    "\n",
    "# #     print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "\n",
    "# def train_network(self, model, criterion, optimizer, training_loader, device, number_of_epochs):\n",
    "model.train()\n",
    "for epoch in range(number_of_epochs):\n",
    "    for input_tensor, label in training_loader:\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        charlm_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass happening here\n",
    "        predictions = model(input_tensor).to(device)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = model.backward_pass(model, predictions, label, learning_rate)\n",
    "\n",
    "    print(\"Epoch\", epoch+1, \"completed : \", end=\"\")\n",
    "    print(\"loss=\", loss)\n",
    "\n",
    "\n",
    "# def backward_pass(self, x, y):\n",
    "#     loss = self.criterion(x.squeeze(1), y)\n",
    "#     loss.backward()\n",
    "#     self.optimizer.step()\n",
    "#     return loss.detach().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427a50-617e-476b-b3b3-ddfbac543286",
   "metadata": {},
   "source": [
    "Check how well the model works by entering a string and letting the model generate the continuation of that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a3903-5bc9-4a8e-8299-fcb54f037f95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ker an the remke was ou he wile were ang roud the nod ar were surery wir an the wirere war  an ory f rileat an the remere wan he for the Radde sook the vere seing of the bilkage on the Riddles had be\n",
      " ar  he rad an the the cow was the we war war on the vireage an the Riddles and at nar ous the ridlan the had lee wusee sar bous the had an the ranked of the Riddle House.  \"rSed and come boc, wor an \n"
     ]
    }
   ],
   "source": [
    "charlm.eval()\n",
    "while True :\n",
    "    start = input(\">\")\n",
    "    if start.strip() == 'quit' :\n",
    "        break\n",
    "    # Add spaces in case the start string is too short\n",
    "    start = ' '*(n-len(start)) + start\n",
    "    # Ignore everything but the last n characters of the start string\n",
    "    ids = [char_to_id[c] for c in start][-n:]\n",
    "    # Generate 200 characters starting from the start string\n",
    "    try:\n",
    "        for i in range(200):\n",
    "            input_tensor = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character_id = new_character_tensor.detach().item()\n",
    "            print(id_to_char[new_character_id], end='')\n",
    "            ids.pop(0)\n",
    "            ids.append(new_character_id)\n",
    "        print()\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0628b2-3408-4e40-a01a-4fe94efde5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
