{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f712f8-daa3-4dbd-9360-5d9d932c490e",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "The point of the exercise is to construct a simple neural character model that can predict the (n+1)th character, given the n preceding characters.\n",
    "\n",
    "Usually, such language models operate on the word level, but we use a character model because it is simpler and quicker to train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bf371b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/imrandiva/Library/Python/3.10/lib/python/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f1aeaf0-3a25-4467-b4d0-61535b30bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b035e-b525-432a-855f-de109266da05",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before.\n",
    "\n",
    "For instance, if the text begins \"Harry Potter\", we want to transform this into $[1, 2, 3, 3, 4, 5, 6, 7, 8, 8, 9, 3, ...]$, where \"H\" has ID 1, \"a\" is 2, \"r\" is 3, etc. (ID 0 is reserved for the special padding symbol, so we start numbering from 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "63e58f36-84ad-45a7-9260-8e714a1a2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to init mappings from characters to IDs and back again\n",
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0  # ID 0 is reserved for <PAD>\n",
    "id_to_char.append(PADDING_SYMBOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e81d0246-7a35-446e-8cf6-33e130b545c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this function \n",
    "def string_to_ids(string) :\n",
    "    \"\"\"\n",
    "    Translate this string into a list of character IDs.\n",
    "    The IDs will be integers 1,2,..., and created as needed.\n",
    "    \"\"\"\n",
    "    chars_ids = []  # This list will hold the result \n",
    "\n",
    "    for char in string:\n",
    "        # YOUR CODE HERE\n",
    "        if char not in char_to_id:\n",
    "            char_to_id[char] = len(id_to_char)\n",
    "            id_to_char.append(char)\n",
    "        chars_ids.append(char_to_id[char])\n",
    "\n",
    "    return chars_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2e94e393-2e80-4980-8fe7-fc1b766a3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "with open('goblet_book.txt', 'r', encoding='utf-8') as f:\n",
    "    contents = f.read() \n",
    "chars_ids = string_to_ids(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ebae1-9f82-4c44-8f36-2dc936f3f785",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.\n",
    "\n",
    "The init function reads a training text, and slides over it, creating chunks $n$ characters long. These chunks will be our data points, and the corresponding $(n+1)$th character will be the label.\n",
    "\n",
    "For instance, if $n=4$, and the text begins \"Harry P\", which corresponds to the IDs $1,2,3,3,4,5,6$, then the first data point will be $[1,2,3,3]$ and its label is $4$, the second data point is $[2,3,3,4]$ with label $5$, and the third data point is $[3,3,4,5]$ with label $6$.\n",
    "\n",
    "To extend the 'Dataset' class, the CharDataset class has to implement the __len__ and __getitem__ methods, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2f4d7c88-15d2-47ad-8242-4a94aa826ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this class definition\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    datapoints = []  # Each datapoint is a sequence of n characters\n",
    "    labels = []  # The corresponding label is the character that comes next\n",
    "\n",
    "    def __init__(self, file_path, n):\n",
    "        \"\"\"\n",
    "        'file_path' is the name of the training data file\n",
    "\n",
    "        'n' is the number of consecutive characters the model will look at\n",
    "        to predict which letter comes next\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read() \n",
    "        chars_ids = string_to_ids(contents)\n",
    "\n",
    "        # Go through the chars_ids and create data points and labels\n",
    "        # YOUR CODE HERE\n",
    "        for i in range(len(chars_ids)-n-1):\n",
    "            self.datapoints.append(chars_ids[i:i+n])\n",
    "            self.labels.append(chars_ids[i+n])\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f71171f5-5c9c-4a6c-b5ab-f7112daba56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def char_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the longest sequence in the batch.\n",
    "\n",
    "    'batch' is a list of tuples [(datapoint, label), ...]\n",
    "\n",
    "    Returns a tuple of:\n",
    "            - Padded datapoints as a tensor\n",
    "            - Labels as a tensor \n",
    "   \"\"\"\n",
    "    datapoints, labels = zip(*batch)  \n",
    "    padded_datapoints = pad_sequence(datapoints, batch_first=True)\n",
    "    return padded_datapoints, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e6c95-d1bd-4383-8e54-ce0ec1158130",
   "metadata": {},
   "source": [
    "#### Create a neural network according to the following specification:\n",
    "\n",
    "The hyperparameters are:\n",
    "* n -- the number of characters to input (to predict character n+1)\n",
    "* h -- the number of neurons in the hidden layer\n",
    "* v -- the number of unique characters\n",
    "\n",
    "The network should have:\n",
    "1. an embedding layer, mapping character IDs to h-dimensional vectors\n",
    "2. a hidden layer with a linear transformation of size $(nh)\\times(nh)$, followed by a tanh application\n",
    "3. a final layer with a linear transformation of size $(nh)\\times v$\n",
    "\n",
    "The input to the forward pass is a tensor of character IDs $x$ of shape $(\\mathtt{batch\\_size} \\times n)$. The forward pass should:\n",
    "1. Map $x$ to a tensor of character embeddings of shape $(\\mathtt{batch\\_size} \\times n \\times h)$\n",
    "2. Reshape that tensor to shape $(\\mathtt{batch\\_size} \\times nh)$\n",
    "3. Apply the hidden layer (linear transformation and the tanh operation)\n",
    "4. Apply the final layer\n",
    "5. Return the result of the last operation\n",
    "\n",
    "Before starting the implementation, have a look at the documentation for:\n",
    " - `torch.nn.Embedding`\n",
    " - `torch.nn.Linear`\n",
    " - `torch.tanh`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "392ba45c-1a18-4e81-8d7f-7ea3d9f33470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n, h, v):\n",
    "        super(CharModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.v = v\n",
    "        self.h = h\n",
    "        self.embed = nn.Embedding(v, h)\n",
    "        self.rnn = nn.RNN(h, h, batch_first=True) \n",
    "        self.final = nn.Linear(n*h, v)\n",
    "\n",
    "    def forward(self, x, h0= None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.rnn(x, h0)  # RNN layer\n",
    "        out = out.reshape(batch_size, self.n*self.h)\n",
    "        out = self.final(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bff77c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n, h, v):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.v = v\n",
    "        self.h = h\n",
    "        self.embed = nn.Embedding(v, h)\n",
    "        self.lstm = nn.LSTM(h, h, batch_first=True) \n",
    "        self.final = nn.Linear(n*h, v)\n",
    "\n",
    "    def forward(self, x, h0= None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.embed(x)\n",
    "        out, _ = self.lstm(x, h0) # LSTM layer\n",
    "        out = out.reshape(batch_size, self.n*self.h)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    "\n",
    "    def backward_pass(self, model, x, y, lr):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss = criterion(x.squeeze(1), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.detach().item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7192cd-6e9a-4cb1-8840-ca5973658f0d",
   "metadata": {},
   "source": [
    "Next, we will train a model with n=8, i.e. the model will try to predict the 9th character based on the 8 preceding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1efb5e1f-1f4c-4d1f-aee1-1f1857f98aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "There are 3187 datapoints and 58 unique characters in the dataset\n",
      "Epoch 1 completed : loss= 2.9330852031707764\n",
      "Epoch 2 completed : loss= 2.4685280323028564\n",
      "Epoch 3 completed : loss= 2.4316422939300537\n",
      "Epoch 4 completed : loss= 2.3008804321289062\n",
      "Epoch 5 completed : loss= 2.4732511043548584\n"
     ]
    }
   ],
   "source": [
    "# Choose 'Run all cells' in the 'Run' menu to run this cell.\n",
    "_ = torch.manual_seed(21)\n",
    "\n",
    "# ===================== Hyperparameters ================== #\n",
    "\n",
    "n = 32\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "number_of_epochs = 5\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "training_dataset = CharDataset('goblet_book.txt', n)\n",
    "print( \"There are\", len(training_dataset), \"datapoints and\", len(id_to_char), \"unique characters in the dataset\" ) \n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, collate_fn=char_collate_fn, shuffle=True)\n",
    "model = LSTMModel(n, hidden_size, len(char_to_id)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# model.train()\n",
    "# # print(datetime.now().strftime(\"%X\"), \"Training starts\")\n",
    "# # for epoch in range(number_of_epochs) :\n",
    "# #     for input_tensor, label in training_loader:\n",
    "# #         input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "\n",
    "# #         charlm_optimizer.zero_grad()\n",
    "\n",
    "# #         # Forward pass happening here\n",
    "# #         logits = model(input_tensor).to(device)\n",
    "        \n",
    "# #         # Compute the loss\n",
    "# #         loss = criterion(logits.squeeze(1), label)\n",
    "# #         loss.backward()\n",
    "\n",
    "# #         # Update the weights\n",
    "# #         charlm_optimizer.step()\n",
    "\n",
    "\n",
    "# #     print( datetime.now().strftime(\"%X\"), \"End of epoch\", epoch+1, \", loss=\", loss.detach().item())\n",
    "\n",
    "# def train_network(self, model, criterion, optimizer, training_loader, device, number_of_epochs):\n",
    "model.train()\n",
    "for epoch in range(number_of_epochs):\n",
    "    for input_tensor, label in training_loader:\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        charlm_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass happening here\n",
    "        predictions = model(input_tensor).to(device)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = model.backward_pass(model, predictions, label, learning_rate)\n",
    "\n",
    "    print(\"Epoch\", epoch+1, \"completed : \", end=\"\")\n",
    "    print(\"loss=\", loss)\n",
    "\n",
    "\n",
    "# def backward_pass(self, x, y):\n",
    "#     loss = self.criterion(x.squeeze(1), y)\n",
    "#     loss.backward()\n",
    "#     self.optimizer.step()\n",
    "#     return loss.detach().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427a50-617e-476b-b3b3-ddfbac543286",
   "metadata": {},
   "source": [
    "Check how well the model works by entering a string and letting the model generate the continuation of that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c5a3903-5bc9-4a8e-8299-fcb54f037f95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m         new_character_id \u001b[38;5;241m=\u001b[39m new_character_tensor\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(id_to_char[new_character_id], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(new_character_id)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "while True :\n",
    "    start = input(\">\")\n",
    "    if start.strip() == 'quit' :\n",
    "        break\n",
    "    # Add spaces in case the start string is too short\n",
    "    start = ' '*(n-len(start)) + start\n",
    "    # Ignore everything but the last n characters of the start string\n",
    "    ids = [char_to_id[c] for c in start][-n:]\n",
    "    # Generate 200 characters starting from the start string\n",
    "    try:\n",
    "        for i in range(200):\n",
    "            input_tensor = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "            logits = model(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character_id = new_character_tensor.detach().item()\n",
    "            print(id_to_char[new_character_id], end='')\n",
    "            ids.pop(0)\n",
    "            ids.append(new_character_id)\n",
    "        print()\n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0628b2-3408-4e40-a01a-4fe94efde5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
